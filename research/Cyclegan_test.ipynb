{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary.torchsummary import summary\n",
    "import torch.nn as nn \n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from networks.discriminator import PatchDiscriminator\n",
    "from networks.generator import ResNetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(image_type, image_dir='summer2winter_yosemite', \n",
    "                    image_size=128, batch_size=16, num_workers=0):\n",
    "    \"\"\"Returns training and test data loaders for a given image type, either 'summer' or 'winter'. \n",
    "       These images will be resized to 128x128x3, by default, converted into Tensors, and normalized.\n",
    "    \"\"\"\n",
    "    \n",
    "    # resize and normalize the images\n",
    "    transform = transforms.Compose([transforms.Resize(image_size), # resize to 128x128\n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "    # get training and test directories\n",
    "    image_path = './' + image_dir\n",
    "    train_path = os.path.join(image_path, image_type)\n",
    "    test_path = os.path.join(image_path, 'test_{}'.format(image_type))\n",
    "\n",
    "    # define datasets using ImageFolder\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
    "\n",
    "    # create and return DataLoaders\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/summer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-01394bcbf1fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataloader_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'summer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataloader_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'winter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-3dd25f88717d>\u001b[0m in \u001b[0;36mget_data_loader\u001b[0;34m(image_type, image_dir, image_size, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# define datasets using ImageFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Program/ML_practice/data-venv/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Program/ML_practice/data-venv/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     91\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     92\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Program/ML_practice/data-venv/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/summer'"
     ]
    }
   ],
   "source": [
    "dataloader_X, test_dataloader_X = get_data_loader(image_type='summer', image_dir='datasets')\n",
    "dataloader_Y, test_dataloader_Y = get_data_loader(image_type='winter', image_dir='datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f8d9019ab26e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# get some images from X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# the \"_\" is a placeholder for no labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader_X' is not defined"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "\n",
    "# get some images from X\n",
    "dataiter = iter(dataloader_X)\n",
    "# the \"_\" is a placeholder for no labels\n",
    "images, _ = dataiter.next()\n",
    "\n",
    "# show images\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader_Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-68d75a94700f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# show images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader_Y' is not defined"
     ]
    }
   ],
   "source": [
    "dataiter = iter(dataloader_Y)\n",
    "images, _ = dataiter.next()\n",
    "\n",
    "# show images\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(g_conv_dim=64, d_conv_dim=64, n_res_blocks=6):\n",
    "    # Instantiate generators\n",
    "    G_XtoY = ResNetGenerator(conv_dim=g_conv_dim, c_dim=3, repeat_num=n_res_blocks)\n",
    "    G_YtoX = ResNetGenerator(conv_dim=g_conv_dim, c_dim=3, repeat_num=n_res_blocks)\n",
    "    # Instantiate discriminators\n",
    "    D_X = PatchDiscriminator(c_dim=3, conv_dim=d_conv_dim)\n",
    "    D_Y = PatchDiscriminator(c_dim=3, conv_dim=d_conv_dim)\n",
    "\n",
    "    # move models to GPU, if available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        G_XtoY.to(device)\n",
    "        G_YtoX.to(device)\n",
    "        D_X.to(device)\n",
    "        D_Y.to(device)\n",
    "        print('Models moved to GPU.')\n",
    "    else:\n",
    "        print('Only CPU available.')\n",
    "\n",
    "    return G_XtoY, G_YtoX, D_X, D_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only CPU available.\n"
     ]
    }
   ],
   "source": [
    "G_XtoY, G_YtoX, D_X, D_Y = create_model(g_conv_dim=64, d_conv_dim=64, n_res_blocks=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           9,408\n",
      "            Conv2d-2         [-1, 64, 128, 128]           9,408\n",
      "    InstanceNorm2d-3         [-1, 64, 128, 128]             128\n",
      "    InstanceNorm2d-4         [-1, 64, 128, 128]             128\n",
      "              ReLU-5         [-1, 64, 128, 128]               0\n",
      "              ReLU-6         [-1, 64, 128, 128]               0\n",
      "            Conv2d-7          [-1, 128, 64, 64]          73,728\n",
      "            Conv2d-8          [-1, 128, 64, 64]          73,728\n",
      "    InstanceNorm2d-9          [-1, 128, 64, 64]             256\n",
      "   InstanceNorm2d-10          [-1, 128, 64, 64]             256\n",
      "             ReLU-11          [-1, 128, 64, 64]               0\n",
      "             ReLU-12          [-1, 128, 64, 64]               0\n",
      "DownsamplingBlock-13          [-1, 128, 64, 64]               0\n",
      "DownsamplingBlock-14          [-1, 128, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 32, 32]         294,912\n",
      "           Conv2d-16          [-1, 256, 32, 32]         294,912\n",
      "   InstanceNorm2d-17          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-18          [-1, 256, 32, 32]             512\n",
      "             ReLU-19          [-1, 256, 32, 32]               0\n",
      "             ReLU-20          [-1, 256, 32, 32]               0\n",
      "DownsamplingBlock-21          [-1, 256, 32, 32]               0\n",
      "DownsamplingBlock-22          [-1, 256, 32, 32]               0\n",
      "           Conv2d-23          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-24          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-25          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-26          [-1, 256, 32, 32]             512\n",
      "             ReLU-27          [-1, 256, 32, 32]               0\n",
      "             ReLU-28          [-1, 256, 32, 32]               0\n",
      "           Conv2d-29          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-30          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-31          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-32          [-1, 256, 32, 32]             512\n",
      "    ResidualBlock-33          [-1, 256, 32, 32]               0\n",
      "    ResidualBlock-34          [-1, 256, 32, 32]               0\n",
      "           Conv2d-35          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-36          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-37          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-38          [-1, 256, 32, 32]             512\n",
      "             ReLU-39          [-1, 256, 32, 32]               0\n",
      "             ReLU-40          [-1, 256, 32, 32]               0\n",
      "           Conv2d-41          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-42          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-43          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-44          [-1, 256, 32, 32]             512\n",
      "    ResidualBlock-45          [-1, 256, 32, 32]               0\n",
      "    ResidualBlock-46          [-1, 256, 32, 32]               0\n",
      "           Conv2d-47          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-48          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-49          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-50          [-1, 256, 32, 32]             512\n",
      "             ReLU-51          [-1, 256, 32, 32]               0\n",
      "             ReLU-52          [-1, 256, 32, 32]               0\n",
      "           Conv2d-53          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-54          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-55          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-56          [-1, 256, 32, 32]             512\n",
      "    ResidualBlock-57          [-1, 256, 32, 32]               0\n",
      "    ResidualBlock-58          [-1, 256, 32, 32]               0\n",
      "           Conv2d-59          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-60          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-61          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-62          [-1, 256, 32, 32]             512\n",
      "             ReLU-63          [-1, 256, 32, 32]               0\n",
      "             ReLU-64          [-1, 256, 32, 32]               0\n",
      "           Conv2d-65          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-66          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-67          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-68          [-1, 256, 32, 32]             512\n",
      "    ResidualBlock-69          [-1, 256, 32, 32]               0\n",
      "    ResidualBlock-70          [-1, 256, 32, 32]               0\n",
      "           Conv2d-71          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-72          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-73          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-74          [-1, 256, 32, 32]             512\n",
      "             ReLU-75          [-1, 256, 32, 32]               0\n",
      "             ReLU-76          [-1, 256, 32, 32]               0\n",
      "           Conv2d-77          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-78          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-79          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-80          [-1, 256, 32, 32]             512\n",
      "    ResidualBlock-81          [-1, 256, 32, 32]               0\n",
      "    ResidualBlock-82          [-1, 256, 32, 32]               0\n",
      "           Conv2d-83          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-84          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-85          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-86          [-1, 256, 32, 32]             512\n",
      "             ReLU-87          [-1, 256, 32, 32]               0\n",
      "             ReLU-88          [-1, 256, 32, 32]               0\n",
      "           Conv2d-89          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-90          [-1, 256, 32, 32]         589,824\n",
      "   InstanceNorm2d-91          [-1, 256, 32, 32]             512\n",
      "   InstanceNorm2d-92          [-1, 256, 32, 32]             512\n",
      "    ResidualBlock-93          [-1, 256, 32, 32]               0\n",
      "    ResidualBlock-94          [-1, 256, 32, 32]               0\n",
      "ResNetGenerator_encoder-95          [-1, 256, 32, 32]               0\n",
      "ResNetGenerator_encoder-96          [-1, 256, 32, 32]               0\n",
      "  ConvTranspose2d-97          [-1, 128, 64, 64]         294,912\n",
      "  ConvTranspose2d-98          [-1, 128, 64, 64]         294,912\n",
      "   InstanceNorm2d-99          [-1, 128, 64, 64]             256\n",
      "  InstanceNorm2d-100          [-1, 128, 64, 64]             256\n",
      "            ReLU-101          [-1, 128, 64, 64]               0\n",
      "            ReLU-102          [-1, 128, 64, 64]               0\n",
      " UpsamplingBlock-103          [-1, 128, 64, 64]               0\n",
      " UpsamplingBlock-104          [-1, 128, 64, 64]               0\n",
      " ConvTranspose2d-105         [-1, 64, 128, 128]          73,728\n",
      " ConvTranspose2d-106         [-1, 64, 128, 128]          73,728\n",
      "  InstanceNorm2d-107         [-1, 64, 128, 128]             128\n",
      "  InstanceNorm2d-108         [-1, 64, 128, 128]             128\n",
      "            ReLU-109         [-1, 64, 128, 128]               0\n",
      "            ReLU-110         [-1, 64, 128, 128]               0\n",
      " UpsamplingBlock-111         [-1, 64, 128, 128]               0\n",
      " UpsamplingBlock-112         [-1, 64, 128, 128]               0\n",
      "          Conv2d-113          [-1, 3, 128, 128]           9,408\n",
      "          Conv2d-114          [-1, 3, 128, 128]           9,408\n",
      "            Tanh-115          [-1, 3, 128, 128]               0\n",
      "            Tanh-116          [-1, 3, 128, 128]               0\n",
      "ResNetGenerator_decoder-117          [-1, 3, 128, 128]               0\n",
      "ResNetGenerator_decoder-118          [-1, 3, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 15,682,816\n",
      "Trainable params: 15,682,816\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 342.25\n",
      "Params size (MB): 59.83\n",
      "Estimated Total Size (MB): 402.26\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(G_YtoX, (3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_mse_loss(D_out):\n",
    "    # how close is the produced output from being \"real\"?\n",
    "    return torch.mean((D_out-1)**2)\n",
    "\n",
    "def fake_mse_loss(D_out):\n",
    "    # how close is the produced output from being \"fake\"?\n",
    "    return torch.mean(D_out**2)\n",
    "\n",
    "def cycle_consistency_loss(real_im, reconstructed_im, lambda_weight):\n",
    "    # calculate reconstruction loss \n",
    "    # as absolute value difference between the real and reconstructed images\n",
    "#     reconstr_loss = torch.mean(torch.abs(real_im - reconstructed_im))\n",
    "    reconstr_loss = torch.nn.L1Loss()\n",
    "    # return weighted loss\n",
    "    return lambda_weight*reconstr_loss(real_im, reconstructed_im)\n",
    "\n",
    "def l2_loss(real_im, reconstructed_im, lambda_weight):\n",
    "    reconstr_loss = torch.nn.MSELoss()\n",
    "    return reconstr_loss(real_im, reconstructed_im)*lambda_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle_consistency_loss(torch.tensor([1.,2.,5.]), torch.tensor([1.,2.,3.]), 5)\n",
    "# torch.mean(torch.tensor([[1.,2.,5.], [1.,2.,5.]]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr=0.0002\n",
    "beta1=0.5\n",
    "beta2=0.999\n",
    "\n",
    "g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters\n",
    "\n",
    "# Create optimizers for the generators and discriminators\n",
    "g_optimizer = optim.Adam(g_params, lr, [beta1, beta2])\n",
    "d_x_optimizer = optim.Adam(D_X.parameters(), lr, [beta1, beta2])\n",
    "d_y_optimizer = optim.Adam(D_Y.parameters(), lr, [beta1, beta2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x, feature_range=(-1, 1)):\n",
    "    ''' Scale takes in an image x and returns that image, scaled\n",
    "       with a feature_range of pixel values from -1 to 1. \n",
    "       This function assumes that the input x is already scaled from 0-1.'''\n",
    "    \n",
    "    # scale from 0-1 to feature_range\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, \n",
    "                  n_epochs=1000):\n",
    "    \n",
    "    print_every=10\n",
    "    \n",
    "    # keep track of losses over time\n",
    "    losses = []\n",
    "    \n",
    "    test_iter_X = iter(test_dataloader_X)\n",
    "    test_iter_Y = iter(test_dataloader_Y)\n",
    "\n",
    "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
    "    # constant throughout training, that allow us to inspect the model's performance.\n",
    "    fixed_X = test_iter_X.next()[0]\n",
    "    fixed_Y = test_iter_Y.next()[0]\n",
    "    fixed_X = scale(fixed_X) # make sure to scale to a range -1 to 1\n",
    "    fixed_Y = scale(fixed_Y)\n",
    "\n",
    "    # batches per epoch\n",
    "    iter_X = iter(dataloader_X)\n",
    "    iter_Y = iter(dataloader_Y)\n",
    "    batches_per_epoch = min(len(iter_X), len(iter_Y))\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs+1)):\n",
    "\n",
    "        # Reset iterators for each epoch\n",
    "        if epoch % batches_per_epoch == 0:\n",
    "            iter_X = iter(dataloader_X)\n",
    "            iter_Y = iter(dataloader_Y)\n",
    "\n",
    "        images_X, _ = iter_X.next()\n",
    "        images_X = scale(images_X) # make sure to scale to a range -1 to 1\n",
    "\n",
    "        images_Y, _ = iter_Y.next()\n",
    "        images_Y = scale(images_Y)\n",
    "        \n",
    "        # move images to GPU if available (otherwise stay on CPU)\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        images_X = images_X.to(device)\n",
    "        images_Y = images_Y.to(device)\n",
    "\n",
    "\n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATORS\n",
    "        # ============================================\n",
    "\n",
    "        ##   First: D_X, real and fake loss components   ##\n",
    "\n",
    "        # Train with real images\n",
    "        d_x_optimizer.zero_grad()\n",
    "\n",
    "        # 1. Compute the discriminator losses on real images\n",
    "        out_x = D_X(images_X)\n",
    "        D_X_real_loss = real_mse_loss(out_x)\n",
    "        \n",
    "        # Train with fake images\n",
    "        \n",
    "        # 2. Generate fake images that look like domain X based on real images in domain Y\n",
    "        fake_X = G_YtoX(images_Y)\n",
    "\n",
    "        # 3. Compute the fake loss for D_X\n",
    "        out_x = D_X(fake_X)\n",
    "        D_X_fake_loss = fake_mse_loss(out_x)\n",
    "        \n",
    "\n",
    "        # 4. Compute the total loss and perform backprop\n",
    "        d_x_loss = D_X_real_loss + D_X_fake_loss\n",
    "        d_x_loss.backward()\n",
    "        d_x_optimizer.step()\n",
    "\n",
    "        \n",
    "        ##   Second: D_Y, real and fake loss components   ##\n",
    "        \n",
    "        # Train with real images\n",
    "        d_y_optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Compute the discriminator losses on real images\n",
    "        out_y = D_Y(images_Y)\n",
    "        D_Y_real_loss = real_mse_loss(out_y)\n",
    "        \n",
    "        # Train with fake images\n",
    "\n",
    "        # 2. Generate fake images that look like domain Y based on real images in domain X\n",
    "        fake_Y = G_XtoY(images_X)\n",
    "\n",
    "        # 3. Compute the fake loss for D_Y\n",
    "        out_y = D_Y(fake_Y)\n",
    "        D_Y_fake_loss = fake_mse_loss(out_y)\n",
    "\n",
    "        # 4. Compute the total loss and perform backprop\n",
    "        d_y_loss = D_Y_real_loss + D_Y_fake_loss\n",
    "        d_y_loss.backward()\n",
    "        d_y_optimizer.step()\n",
    "\n",
    "\n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATORS\n",
    "        # =========================================\n",
    "\n",
    "        ##    First: generate fake X images and reconstructed Y images    ##\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        # 1. Generate fake images that look like domain X based on real images in domain Y\n",
    "        fake_X = G_YtoX(images_Y)\n",
    "\n",
    "        # 2. Compute the generator loss based on domain X\n",
    "        out_x = D_X(fake_X)\n",
    "        g_YtoX_loss = real_mse_loss(out_x)\n",
    "\n",
    "        # 3. Create a reconstructed y\n",
    "        # 4. Compute the cycle consistency loss (the reconstruction loss)\n",
    "        reconstructed_Y = G_XtoY(fake_X)\n",
    "        reconstructed_y_loss = cycle_consistency_loss(images_Y, reconstructed_Y, lambda_weight=10)\n",
    "\n",
    "\n",
    "        ##    Second: generate fake Y images and reconstructed X images    ##\n",
    "\n",
    "        # 1. Generate fake images that look like domain Y based on real images in domain X\n",
    "        fake_Y = G_XtoY(images_X)\n",
    "\n",
    "        # 2. Compute the generator loss based on domain Y\n",
    "        out_y = D_Y(fake_Y)\n",
    "        g_XtoY_loss = real_mse_loss(out_y)\n",
    "\n",
    "        # 3. Create a reconstructed x\n",
    "        # 4. Compute the cycle consistency loss (the reconstruction loss)\n",
    "        reconstructed_X = G_YtoX(fake_Y)\n",
    "        reconstructed_x_loss = cycle_consistency_loss(images_X, reconstructed_X, lambda_weight=10)\n",
    "\n",
    "        # 5. Add up all generator and reconstructed losses and perform backprop\n",
    "        g_total_loss = g_YtoX_loss + g_XtoY_loss + reconstructed_y_loss + reconstructed_x_loss\n",
    "        g_total_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "\n",
    "        # Print the log info\n",
    "        if epoch % print_every == 0:\n",
    "            # append real and fake discriminator losses and the generator loss\n",
    "            losses.append((d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "            print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f} | d_Y_loss: {:6.4f} | g_total_loss: {:6.4f}'.format(\n",
    "                    epoch, n_epochs, d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "\n",
    "            \n",
    "        sample_every=100\n",
    "        # Save the generated samples\n",
    "        if epoch % sample_every == 0:\n",
    "            G_YtoX.eval() # set generators to eval mode for sample generation\n",
    "            G_XtoY.eval()\n",
    "            save_samples(epoch, fixed_Y, fixed_X, G_YtoX, G_XtoY, batch_size=16)\n",
    "            G_YtoX.train()\n",
    "            G_XtoY.train()\n",
    "\n",
    "        # uncomment these lines, if you want to save your model\n",
    "#         checkpoint_every=1000\n",
    "#         # Save the model parameters\n",
    "#         if epoch % checkpoint_every == 0:\n",
    "#             checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "n_epochs = 4000 # keep this small when testing if a model first works\n",
    "\n",
    "losses = training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-venv",
   "language": "python",
   "name": "data-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
